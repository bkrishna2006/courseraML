---
title: "predAssgmt"
author: "BalajiK"
date: "May 14, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Objective

To develop a machine learning algorithm based model to predict the "Classe" variable based on a set of predictor variables. The data is related to personal workout activities and from related monitoring systems. 

## 1. Data preparation

```{r}
# Download the data
# Data source credits :  http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.

Raw_training <- data.frame(read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"),na.strings=c("NA","#DIV/0!",""), header=TRUE))

Raw_testing <- data.frame(read.csv(url("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"),na.strings=c("NA","#DIV/0!",""), header=TRUE))

# Raw_testing <- data.frame(read.csv("C:/Users/balaji_krishnaiyer/Documents/R/Coursera/ML/pml-testing.csv", na.strings = c("NA","#DIV/0!",""), header = TRUE))

# Raw_training <- data.frame(read.csv("C:/Users/balaji_krishnaiyer/Documents/R/Coursera/ML/pml-training.csv", na.strings = c("NA","#DIV/0!",""), header = TRUE))

```

### Many of the columns in the testing dataset are having NA's.  Hence these can be omitted to make the dataset compact. 

### 2. Data Cleansing
```{R}

testing <- (Raw_testing[,colSums(is.na(Raw_testing)) == 0])
training <- (Raw_training[,colSums(is.na(Raw_training)) == 0])

dim(testing); dim(training)
```

### The first 7 columns in the testing & training datasets does not seem very relevant as they contain primarily timestamp/metadata information.  Hence they can be eliminated for model fitting.

```{R}
clean_testing <- testing[,8:60]
clean_training <- training[,8:60]

dim(clean_testing); dim(clean_training)

```

```{r}
summary(clean_training$classe)
```

### The summary function output reveals there are 5 variations of "Classe" variable.  Hence various Clustering algorithms shall be tried out here.. Shall be using the caret package to create Data partitions and run the various algorithms

### 3. Data Partitioning

```{R}
library(caret)
set.seed(51518)

intrain <- createDataPartition(clean_training$classe,p = 0.6, list = FALSE)
part1_training <- clean_training[intrain,]
part2_training <- clean_training[-intrain,]
```
### 4. Model fitting

### Have chosen to try 3 models using rpart, lda and randomForest algorithms and see which one's performing better. 

```{R}
set.seed(51518)

fit_rpart <- train(classe ~., part1_training, method = "rpart")
fit_lda <- train(classe ~., part1_training, method = "lda")
fit_rf <- train(classe ~., part1_training, method = "rf")

```

### 5. Model Validation

### Now these models shall be applied to part2 of the training dataset and validated for the accuracies.

```{R}
pred_rpart <- predict(fit_rpart, part2_training)
pred_lda <- predict(fit_lda, part2_training)
pred_rf <- predict(fit_rf, part2_training)

conf_mtrx_rpart <- confusionMatrix(pred_rpart,part2_training$classe)
conf_mtrx_lda <- confusionMatrix(pred_lda,part2_training$classe)
conf_mtrx_rf <- confusionMatrix(pred_rf,part2_training$classe)

conf_mtrx_rpart$overall
conf_mtrx_lda$overall
conf_mtrx_rf$overall
```
### Comparing the accuracies from the various models, it is observed that Random Forest algorithm based model gives a better accuracy of 98.9 % compared to rpart (49 %) and lda(70 %) algorithm based models.

### 6. Model Application

NOw, as the final step, the chosen model can be applied on the actual test data set to predict the target feature.

```{R}
pred_rf_newdata <- predict(fit_rf, clean_testing)
```

### 7. Conclusion

It is hence concluded that the RF algorithm based model could be used for this prediction use case as it gives the maximum accuracy.